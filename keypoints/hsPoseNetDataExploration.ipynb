{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#imports \n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import argparse\n",
    "import xml.etree.ElementTree\n",
    "import pprint as pp\n",
    "import math\n",
    "from random import shuffle\n",
    "import statistics\n",
    "import shutil\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.io import imread, imshow, imread_collection, concatenate_images\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# <font color='blue'/> Pose ResNet data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Copy over all the XMLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ReadCSVFile:\n",
    "    def __init__(self, csv_file, label_type):\n",
    "        self.csv_file = csv_file\n",
    "        self.column_name = self.get_column_name(label_type)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_column_name(label_type):\n",
    "        if label_type in [\"teethline\", \"teeth\", \"keypoints\"]:\n",
    "            column_name = \"Teeth\"\n",
    "        else:\n",
    "            column_name = \"Scene\"\n",
    "        return column_name\n",
    "\n",
    "    @property\n",
    "    def read_csv(self):\n",
    "        label_folders = []\n",
    "        video_paths = []\n",
    "        equipment_types = []\n",
    "        with open(self.csv_file, newline='') as f:\n",
    "            reader = csv.reader(f, delimiter=',')\n",
    "            first_row = next(reader)\n",
    "            index_label_directory = first_row.index(\n",
    "                \"Custom field ({0} - Image Directory)\".format(self.column_name))\n",
    "            index_is_labeled = first_row.index(\n",
    "                \"Custom field ({0} Images Labeled)\".format(self.column_name))\n",
    "            index_video_path = first_row.index(\"Custom field (Full Path)\")\n",
    "            index_equipment_type = first_row.index(\"Custom field (Equipment Type/Model)\")\n",
    "            for row in reader:\n",
    "                if row[index_is_labeled] == \"Yes\":\n",
    "                    # label_folder = row[index_label_directory].replace('file:////motionmetrics.net/nas/', 'N:/')\n",
    "                    label_folder = row[index_label_directory].replace(\n",
    "                        'file:////motionmetrics.net/nas/', '/home/hooman/')\n",
    "                    video_path = row[index_video_path].replace(\n",
    "                        'file:////motionmetrics.net/nas/', '/home/hooman/')\n",
    "                    equipment_type = row[index_equipment_type]\n",
    "                    video_paths.append(video_path)\n",
    "                    label_folders.append(os.path.normpath(label_folder))\n",
    "                    equipment_types.append(equipment_type)\n",
    "        return label_folders, video_paths, equipment_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def process_teeth_and_wear(root, mpii_json, path2RawImages, save_path, eq_type=\"keypoints\"):\n",
    "    is_wrong = False\n",
    "    for xml_image in root.findall('XMLSaveThumbnail'):\n",
    "        original_image_name = xml_image.get(\"Path\")\n",
    "        exported_image_name = original_image_name.replace(\".png\", \".jpg\")\n",
    "        image = cv2.imread(os.path.join(path2RawImages, exported_image_name))\n",
    "        if image is None:\n",
    "            image = cv2.imread(os.path.join(path2RawImages, original_image_name))\n",
    "            \n",
    "        image, self.x_shift, self.x_aspect_ratio, self.y_aspect_ratio = \\\n",
    "            apply_crop_setting(image, self.camera_type)\n",
    "        mpii_json, is_wrong, image_keypoints = \\\n",
    "            self.extract_keypoints(mpii_json, xml_image, exported_image_name,\n",
    "                                   self.is_filter_state, eq_type)\n",
    "\n",
    "        has_container = xml_image.find(\"HasContainer\").text\n",
    "        is_wrong = is_wrong or not has_container\n",
    "\n",
    "        if not is_wrong:\n",
    "            cv2.imwrite(os.path.join(save_path, exported_image_name), image)\n",
    "\n",
    "            image_debug = image.copy()\n",
    "            self.visualize_img_keypoints(image_debug, image_keypoints,\n",
    "                                         exported_image_name, save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paths\n",
    "csv_file = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/try0_allAnuarStuff/MMI_JIRA_2019-03-18T15_42_45-0700.csv'\n",
    "\n",
    "path2RawImages = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/try2_sameDataAsTry1_butCleanedUpAnnot/images/'\n",
    "\n",
    "path2SaveProcessedLabels = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/try2_sameDataAsTry1_butCleanedUpAnnot/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Copy over the xml labels\n",
    "path2SaveXmls = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/allXMLs/'\n",
    "\n",
    "label_folders, video_paths, equipment_types = ReadCSVFile(csv_file, label_type=\"keypoints\").read_csv\n",
    "\n",
    "for imFolder in label_folders:\n",
    "    newName = path2SaveXmls + imFolder.split('/')[-3] + '.xml'\n",
    "    shutil.copy(os.path.join(imFolder, \"Imageinfo.xml\"), newName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# saves the datain a json file.\n",
    "\n",
    "mpii_json = []\n",
    "\n",
    "for xml_file in os.listdir(path2SaveXmls):\n",
    "    try:\n",
    "        path2SaveAnnots = os.path.join(path2SaveProcessedLabels, \"annot\")\n",
    "        create_folder(path2SaveAnnots)\n",
    "        create_folder(os.path.join(path2SaveProcessedLabels, \"debug\"))\n",
    "        is_wrong = False\n",
    "\n",
    "        tree = xml.etree.ElementTree.parse(xml_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        mpii_json, is_wrong = process_teeth_and_wear(root, mpii_json, path2RawImages, save_path)\n",
    "\n",
    "        print(\"Exporting the images of %s folder is done!\" % image_folder)\n",
    "        if is_wrong:\n",
    "            print(\"Issue %s wrong teeth coords!\" % (image_folder))\n",
    "    except Exception as e:\n",
    "        print(\"======= Issue %s failed!\\n %s =======\" % (image_folder, e))\n",
    "self.save_dataset(mpii_json, val_ratio=0.08)\n",
    "print(\"\\nFinished putting images and json files of %d issues together!\\n\" %\n",
    "      len(folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "    def save_output(self):\n",
    "        \"\"\" Iterates through each issue and then saves the resulting dict in\n",
    "        a json file. \"\"\"\n",
    "        mpii_json = []\n",
    "        folders, videos, eq_types = self.read_csv\n",
    "        for image_folder, video_path, eq_type in zip(folders, videos, eq_types):\n",
    "            try:\n",
    "                save_path = os.path.join(self.output_dir, \"images\")\n",
    "                save_path_annot = os.path.join(self.output_dir, \"annot\")\n",
    "                create_folder(save_path)\n",
    "                create_folder(save_path_annot)\n",
    "                if self.is_debug:\n",
    "                    create_folder(os.path.join(save_path, \"debug\"))\n",
    "                is_wrong = False\n",
    "                if \"keypoints\" in self.label_type:  # NOTE!\n",
    "                    root = self.parse_xml(os.path.join(image_folder, \"Imageinfo.xml\"))\n",
    "                    mpii_json, is_wrong = self.process_teeth_and_wear(root,\n",
    "                                                                      mpii_json,\n",
    "                                                                      image_folder,\n",
    "                                                                      save_path,\n",
    "                                                                      eq_type)\n",
    "                else:\n",
    "                    raise Exception(\"Wrong args.label_type\", self.label_type)\n",
    "\n",
    "                print(\"Exporting the images of %s folder is done!\" % image_folder)\n",
    "                if is_wrong:\n",
    "                    print(\"Issue %s wrong teeth coords!\" % (image_folder))\n",
    "            except Exception as e:\n",
    "                print(\"======= Issue %s failed!\\n %s =======\" % (image_folder, e))\n",
    "        self.save_dataset(mpii_json, val_ratio=0.08)\n",
    "        print(\"\\nFinished putting images and json files of %d issues together!\\n\" %\n",
    "              len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'/> Data Exploration Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images: Copying, Moving, Deleting from different directoreis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# deleting files that are not in one dir from another\n",
    "fileToKeepDict = {}\n",
    "\n",
    "for fileName in os.listdir('/home/hooman/Downloads/MOTIONMETRICS (2)/FM/preds/'):\n",
    "    fileToKeepDict[fileName[:-3]+'gmp'] = 1\n",
    "\n",
    "\n",
    "    \n",
    "for fileName in os.listdir('/home/hooman/Downloads/MOTIONMETRICS (2)/FM/1947EFC0-16D8-1588-A042-3534DFB3FA0F/'):\n",
    "    if fileName not in fileToKeepDict:\n",
    "        print(fileName)\n",
    "        os.remove('/home/hooman/Downloads/MOTIONMETRICS (2)/FM/1947EFC0-16D8-1588-A042-3534DFB3FA0F/' + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# deleting images that cannot be opened (usaully after augmentation)\n",
    "dirToDeleteFrom = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/randomCroppedImages/'\n",
    "\n",
    "for fileName in os.listdir(dirToDeleteFrom):\n",
    "    try:\n",
    "        img = imread(dirToDeleteFrom + fileName)\n",
    "    except:\n",
    "        os.remove(dirToDeleteFrom + fileName)\n",
    "        os.remove('/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/randomCroppedMasks/' + fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# copying files from one dir to another\n",
    "\n",
    "import shutil\n",
    "\n",
    "for imgId in os.listdir('/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/dataCleanup_round1/goodLabels'):\n",
    "    shutil.copy('/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/images/' + imgId, '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/try1_only_dataCleanup_round1_goodImages/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# deleting images from a directory\n",
    "\n",
    "for imgId in os.listdir('/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/try1_only_dataCleanup_round1_goodImages/takeOut'):\n",
    "    os.remove('/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/poseNet/hydraulic/data/try2_sameDataAsTry1_HeavierCleanup/images/' + imgId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Exclusing testSet images from trainingSet for BucketTracking\n",
    "# copying files from one dir to another\n",
    "\n",
    "dirWithListOfimages = \"/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/goodMatInsides_forBBLabelingOfYolo/\"\n",
    "\n",
    "dir2RemoveFrom = \"/media/hooman/hsSsdPartUbuntu/FM_PROJECT/dataPreparation/FMDL_3.1/cable/fmdl-cable-trainingData/images/\"\n",
    "\n",
    "\n",
    "import shutil\n",
    "\n",
    "movedLabels = []\n",
    "\n",
    "for imgId in os.listdir(dirWithListOfimages):\n",
    "    \n",
    "    movedLabels.append(imgId)\n",
    "\n",
    "    imgName = imgId.replace('.jpg', '.xml')\n",
    "    \n",
    "    \n",
    "    if os.path.isfile(dir2RemoveFrom + imgId):\n",
    "    #if os.path.isfile(dir2RemoveFrom + imgName):\n",
    "    \n",
    "        #shutil.move(dir2RemoveFrom + imgName, '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/cable/validationsSet_hsPicked_labels')\n",
    "        \n",
    "        shutil.copy(dir2RemoveFrom + imgId, '/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/temp')\n",
    "    \n",
    "        #os.remove(dir2RemoveFrom + imgId)\n",
    "        #os.remove(dir2RemoveFrom + imgName)\n",
    "        print(dir2RemoveFrom + imgName)\n",
    "        \n",
    "    \n",
    "print(\"\")\n",
    "print(\"Moved labels and deleted images for  \" + str(len(movedLabels))  + \"  examples\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Images: resizing, converting formats and channels, correcting ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Resize and downsample all images to (128, 160, 3)\n",
    "resizedImagesPath = '/home/hooman/dataPreparation/hsTestSet/images0PaddedForUNet/'\n",
    "\n",
    "for fileName in os.listdir(imagesPath):\n",
    "\n",
    "    img = imread(imagesPath + fileName) \n",
    "    \n",
    "    imgResized = cv2.resize(img, (640, 480)) \n",
    "\n",
    "    # you can downsample with numpy too. I use opencv to be consistant.    imgDs = img[::4, ::4, :]\n",
    "    imgDs = cv2.resize(imgResized, (0,0), fx=0.25, fy=0.25) \n",
    "\n",
    "    imgPadded = cv2.copyMakeBorder(imgDs,4,4,0,0,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
    "    \n",
    "    cv2.imwrite(resizedImagesPath + fileName, imgPadded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting single channel images to 3 channels\n",
    "for imId in os.listdir('/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/cable/temp/'):\n",
    "\n",
    "    img = imread('/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/cable/temp/'+ imId)\n",
    "    if len(img.shape) < 3:\n",
    "        img3Chan = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "        cv2.imwrite('/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/dataPreparation/FMDL_3.1/cable/hsTestSetOfHardImages/' + imId, img3Chan)\n",
    "        \n",
    "    else:\n",
    "        print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# converting single channel images to 3 channels (LOOPING OVER MULTIPLE FOLDERS)\n",
    "\n",
    "img_dest_dir = '/home/hooman/Desktop/i2lData_cropped/'\n",
    "\n",
    "\n",
    "\n",
    "for mainDir in os.listdir(img_dest_dir):\n",
    "    \n",
    "\n",
    "    for imId in os.listdir(img_dest_dir + mainDir):\n",
    "\n",
    "        img = imread(img_dest_dir + mainDir + '/' + imId)\n",
    "        if len(img.shape) < 3:\n",
    "            img3Chan = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "            cv2.imwrite(img_dest_dir + mainDir + '/' + imId, img3Chan)\n",
    "\n",
    "        else:\n",
    "            print(img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting jpg image to png, and removing the jpegs\n",
    "img_dest_dir = '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/allImages/'\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "imageFileNameDic = {}\n",
    "\n",
    "for file in os.listdir(img_dest_dir):\n",
    "    fileName = file.replace(\".jpg\", \"\")\n",
    "    #fileName = file.replace(\".png\", \"\")\n",
    "    if fileName in imageFileNameDic:\n",
    "        imageFileNameDic[fileName] += 1\n",
    "    else:\n",
    "        imageFileNameDic[fileName] = 0\n",
    "\n",
    "        \n",
    "        \n",
    "for name in imageFileNameDic.keys():\n",
    "    print(name)\n",
    "    im = Image.open(img_dest_dir + '/' + name + '.jpg')\n",
    "    im.save(img_dest_dir + '/' + name + '.png')\n",
    "    os.remove(img_dest_dir + '/' + name + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Converting jpg image to png, and removing the jpegs (looping over multiple folders)\n",
    "\n",
    "img_dest_dir = '/home/hooman/Desktop/i2lData_cropped/'\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "for mainDir in os.listdir(img_dest_dir):\n",
    "\n",
    "\n",
    "    imageFileNameDic = {}\n",
    "\n",
    "    for file in os.listdir(img_dest_dir + mainDir):\n",
    "        \n",
    "        fileName = file.replace(\".jpg\", \"\")\n",
    "        #fileName = file.replace(\".png\", \"\")\n",
    "        if fileName in imageFileNameDic:\n",
    "            imageFileNameDic[fileName] += 1\n",
    "        else:\n",
    "            imageFileNameDic[fileName] = 0\n",
    "\n",
    "\n",
    "\n",
    "    for name in imageFileNameDic.keys():\n",
    "        print(name)\n",
    "        im = Image.open(img_dest_dir + mainDir + '/' + name + '.jpg')\n",
    "        im.save(img_dest_dir + mainDir + '/' + name + '.png')\n",
    "        os.remove(img_dest_dir + mainDir + '/' + name + '.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# compressing png images with jpeg\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "saveDir = '/home/hooman/FM_PROJECT/dataPreparation/fmdlTestData-optical-hydraulic/compressedJpeg80/'\n",
    "\n",
    "for imId in os.listdir('/home/hooman/FM_PROJECT/dataPreparation/fmdlTestData-optical-hydraulic/Frame/'):\n",
    "    img = Image.open('/home/hooman/FM_PROJECT/dataPreparation/fmdlTestData-optical-hydraulic/Frame/'+ imId)\n",
    "    \n",
    "    fileName = imId.replace(\".png\", \"\")\n",
    "    \n",
    "    img.save(saveDir + '/' + fileName + '.jpg', quality=80,optimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Correcting image Ids by content matching singleImage\n",
    "#HSNOTE: this does not work with abs error must be squared.\n",
    "\n",
    "pathToCorrectNames = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/input-orig/'\n",
    "pathToWrongNames   = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/outputOfSaveH5/'\n",
    "pathToWrongPreds = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/ouputOfNetworkJustOutput/'\n",
    "\n",
    "\n",
    "for srcId in os.listdir(pathToWrongNames):\n",
    "\n",
    "    minScore = 100000000\n",
    "    minId = ''\n",
    "    \n",
    "    for targId in os.listdir(pathToCorrectNames):\n",
    "        srcIm = imread(pathToWrongNames + srcId)\n",
    "\n",
    "        temp= imread(pathToCorrectNames + targId)\n",
    "        targIm = cv2.resize(temp, (srcIm.shape[1], srcIm.shape[0])) \n",
    "\n",
    "        dif = np.square((srcIm - targIm))\n",
    "        score = np.sum(dif)\n",
    "\n",
    "        if score < minScore:\n",
    "            minScore = score\n",
    "            minId = targId\n",
    "            \n",
    "\n",
    "    print(\"src: \" + srcId + \"  matched with: \" + minId)\n",
    "    os.rename(pathToWrongNames + srcId, pathToWrongNames + minId)\n",
    "    os.rename(pathToWrongPreds + srcId, pathToWrongPreds + minId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Correcting image Ids by content matching allChannels\n",
    "\n",
    "\n",
    "pathToCorrectNames = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/input-orig/'\n",
    "pathToWrongNames   = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/outputOfSaveH5/'\n",
    "\n",
    "pathToCorrectNamesIn = '/home/hooman/dataPreparation/testingMahdisNetworkOnMyLatestTestSetForComparison/ouputOfNetworkAllChannels/' \n",
    "\n",
    "namesDic = {}\n",
    "for srcId in os.listdir(pathToWrongNames):\n",
    "\n",
    "    minScore = 100000000\n",
    "    minId = ''\n",
    "    \n",
    "    for targId in os.listdir(pathToCorrectNames):\n",
    "        srcIm = imread(pathToWrongNames + srcId)\n",
    "\n",
    "        temp= imread(pathToCorrectNames + targId)\n",
    "        targIm = cv2.resize(temp, (srcIm.shape[1], srcIm.shape[0])) \n",
    "\n",
    "        dif = np.square((srcIm - targIm))\n",
    "        score = np.sum(dif)\n",
    "\n",
    "        if score < minScore:\n",
    "            minScore = score\n",
    "            minId = targId\n",
    "            \n",
    "    print(srcId + \"___\" + minId)\n",
    "    namesDic[srcId] = minId\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import glob\n",
    "\n",
    "for srcId in namesDic.keys():\n",
    "\n",
    "    #print(\"src: \" + srcId + \"  matched with: \" + minId)\n",
    "    \n",
    "    nameAr = srcId.split('_')\n",
    "    nameAr = nameAr[0:2]\n",
    "\n",
    "    shortName = \"\"\n",
    "    for i in range(len(nameAr)):\n",
    "        shortName = shortName + nameAr[i] + '_'\n",
    "\n",
    "    chanFiles = glob.glob1(pathToCorrectNamesIn, shortName + '*')\n",
    "    \n",
    "    for fil in chanFiles:\n",
    "        chan = fil.split('_')[2]\n",
    "        #print(chan)\n",
    "        \n",
    "        if chan[0:2] == \"ch\":\n",
    "            newname = chan + \"_\" + namesDic[srcId]\n",
    "            print(\"renamed CH: \" + fil + \" to: \" + newname + \"\\n\")\n",
    "            os.rename(pathToCorrectNamesIn + fil, pathToCorrectNamesIn + newname)\n",
    "        else:\n",
    "            newname = namesDic[srcId]\n",
    "            print(\"renamed: \" + fil + \" to: \" + newname + \"\\n\")\n",
    "            os.rename(pathToCorrectNamesIn + fil, pathToCorrectNamesIn + namesDic[srcId])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images: Displaying side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Showing Images side-by-side\n",
    "\n",
    "predsDir1 = '/media/hooman/961293e3-04a5-40c5-afc0-2b205d0a7067/WM_PROJECT/algorithmDev/wmAlgo_usingWearLandmarsk_optical_hydraulics/try1/wmdlLogs_aitik_Komatsu_SH1142_PC5500_2019-02-26_to_2019-03-10/yolo_preds/'\n",
    "\n",
    "predsDir2 = '/media/hooman/961293e3-04a5-40c5-afc0-2b205d0a7067/WM_PROJECT/algorithmDev/wmAlgo_usingWearLandmarsk_optical_hydraulics/try1/wmdlLogs_aitik_Komatsu_SH1142_PC5500_2019-02-26_to_2019-03-10/usingNewPoseNet/yolo_preds/'\n",
    "\n",
    "dirToSaveResults = '/media/hooman/961293e3-04a5-40c5-afc0-2b205d0a7067/WM_PROJECT/algorithmDev/wmAlgo_usingWearLandmarsk_optical_hydraulics/try1/wmdlLogs_aitik_Komatsu_SH1142_PC5500_2019-02-26_to_2019-03-10/usingNewPoseNet/comp_newAndOldResnet/'\n",
    "\n",
    "for imgId in os.listdir(predsDir1):  \n",
    "    if '.png' in imgId:\n",
    "        pred1 = imread(predsDir1 + imgId)\n",
    "        pred2 = imread(predsDir2 + imgId)\n",
    "\n",
    "        try:\n",
    "\n",
    "\n",
    "            combImg = np.zeros((pred1.shape[0],1400, 3), np.uint8)\n",
    "\n",
    "            combImg[:, 0:pred1.shape[1], :] = pred1\n",
    "            combImg[:, 700:pred2.shape[1]+700, :] = pred2\n",
    "\n",
    "            '''\n",
    "            combImg = np.zeros((pred1.shape[0],1400), np.uint8)\n",
    "\n",
    "            combImg[:, 0:pred1.shape[1]] = pred1\n",
    "            combImg[:, 700:pred2.shape[1]+700] = pred2\n",
    "            '''\n",
    "\n",
    "\n",
    "\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            cv2.putText(combImg,'oldPoseNet',(30,70), font, 2,(255,255,255), 2, 0)\n",
    "            cv2.putText(combImg,'newPoseNet',(730,70), font, 2,(255,255,255), 2, 0)\n",
    "\n",
    "\n",
    "            #plt.imshow(combImg)\n",
    "            #plt.show()\n",
    "            #break\n",
    "\n",
    "            cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "        except:\n",
    "            print(imgId)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Showing Images side-by-side for 1280*1280 image sizes \n",
    "\n",
    "\n",
    "#predsDir1 = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/try0-AnuarConfigs/image_hard_pickedByHs_predicted_Anuars_model/'\n",
    "\n",
    "predsDir1 = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/try5-NewTrainingProcedure--higherBatchSize/preds_onBackgroundImages/'\n",
    "\n",
    "\n",
    "predsDir2 = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/try11_sameAs5_afterDataCorrections/preds_onBackground/'\n",
    "\n",
    "dirToSaveResults = '/media/hooman/1tb-ssd-hs3-linu/BucketTracking-Project/hydraulic/try11_sameAs5_afterDataCorrections/combined_try5Vs11_backgroundImages/'\n",
    "\n",
    "for imgId in os.listdir(predsDir1):\n",
    "    #hsPred = imread(hsPredsDir + imgId)\n",
    "    #temp = imread(vesPredsDir + imgId)\n",
    "    #vesPred = img3Chan = cv2.cvtColor(temp, cv2.COLOR_GRAY2BGR) \n",
    "    \n",
    "    pred1 = imread(predsDir1 + imgId)\n",
    "    pred2 = imread(predsDir2 + imgId)\n",
    "\n",
    "    combImg = np.zeros((pred1.shape[0],2560, 3), np.uint8)\n",
    "\n",
    "    combImg[:, 0:pred1.shape[1], :] = pred1\n",
    "    combImg[:, 1280:pred2.shape[1]+1280, :] = pred2\n",
    "    \n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(combImg,'try5',(30,70), font, 2,(255,255,255), 2, 0)\n",
    "    cv2.putText(combImg,'try11',(730,70), font, 2,(255,255,255), 2, 0)\n",
    "    \n",
    "    #plt.imshow(combImg)\n",
    "    #plt.show()\n",
    "    #break\n",
    "    \n",
    "    cv2.imwrite(dirToSaveResults + imgId, combImg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Putting the optical flow and U-Net outputs sidebyside.  \n",
    "\n",
    "#FMDL_2018.04.30_11.38.09.png\n",
    "\n",
    "temp = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/Frame/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "frame = cv2.cvtColor(temp, cv2.COLOR_GRAY2BGR) \n",
    "\n",
    "plt.imshow(frame)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "of = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/OpticalFlowMagnitude/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "plt.imshow(of)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "no = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/NetOut/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "plt.imshow(no)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "temp2 = imread('/home/hooman/dataPreparation/fmdlTestData-optical-hydraulic/VES_finalOutput/fragmentation_results/all/' + 'FMDL_2018.04.30_11.38.09.png')\n",
    "\n",
    "fo = cv2.cvtColor(temp2, cv2.COLOR_GRAY2BGR) \n",
    "\n",
    "plt.imshow(fo)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(np.amax(of))\n",
    "\n",
    "\n",
    "combImg = np.zeros((frame.shape[0],2800, 3), np.uint8)\n",
    "\n",
    "combImg[:, 0:frame.shape[1], :] = frame\n",
    "combImg[:, 700:frame.shape[1]+700, :] = cv2.resize(cv2.cvtColor(of, cv2.COLOR_GRAY2BGR), (frame.shape[1], frame.shape[0])) \n",
    "combImg[:, 1400:frame.shape[1]+1400, :] = cv2.resize(cv2.cvtColor(no, cv2.COLOR_GRAY2BGR), (frame.shape[1], frame.shape[0])) \n",
    "combImg[:, 2100:fo.shape[1]+2100, :] = fo\n",
    "\n",
    "\n",
    "plt.imshow(combImg)\n",
    "plt.show()\n",
    "cv2.imwrite('/home/hooman/' + 'combined_FMDL_2018.04.30_11.38.09.png', combImg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# shuffling a csv  (adds an empty line somewhere, and moves the header)\n",
    "\n",
    "csvRows = readCsvRows('/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/backhoe/boxDetector_V2_multiclass/try2-withCaseObject-newData/trainingSet.csv')\n",
    "\n",
    "\n",
    "\n",
    "# shuffle the rows\n",
    "from random import shuffle\n",
    "shuffle(csvRows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#write the shuffled rows to csv\n",
    "csv_file = open('/media/hooman/hsSsdPartUbuntu/FM_PROJECT/FMDL_3.1/backhoe/boxDetector_V2_multiclass/try2-withCaseObject-newData/trainingSet_shuffled.csv', \"w\") \n",
    "\n",
    "\n",
    "# write rows\n",
    "for row in csvRows:\n",
    "    csv_file.write(row + '\\n')\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"wrote \" + str(len(csvRows)) + \" rows to csv file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "#read rows from the file you wanna append to\n",
    "existingRowsDic = getCertainClassRowsDictFromCsv('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try5/trainSet_multiClass_bucket_fineRock_try5_manuallyCleaned.csv', ['matInside'])\n",
    "\n",
    "\n",
    "\n",
    "#read rows from the file you wanna get the new rows from\n",
    "rowsDicToAddFrom = getCertainClassRowsDictFromCsv('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/unusedCsvFiles/trainSet_multiClass_bucket_fineRockInapp_try3_uncleaned.csv', ['matInside'])\n",
    "\n",
    "\n",
    "\n",
    "#Append the missing rows\n",
    "n = 0\n",
    "rowsDicToAddTo = {}\n",
    "for imId in os.listdir('/home/hooman/dataPreparation/hsTrainingSet/imsToAddMatInsideFor/'):\n",
    "    n += 1\n",
    "    print(imId)\n",
    "    if imId not in existingRowsDic:\n",
    "        if imId in rowsDicToAddFrom:\n",
    "            rowsDicToAddTo[imId] = rowsDicToAddFrom[imId]\n",
    "        else:\n",
    "            print(\"error didn't find:  \" + imId + \"\\n\")\n",
    "    else:\n",
    "        print(\"already there\\n\")\n",
    "\n",
    "print(\"processed \" + str(n) + \" rows\")\n",
    "\n",
    "writeRowDicToCsv(rowsDicToAddTo, '/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try4/trainSet_multiClass_bucket_fineRock_try4_manuallyCleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# delete images from csv\n",
    "imIdsToDelete = os.listdir('/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/examplesToRemove/')\n",
    "\n",
    "csvToWorkWith = '/media/hooman/1tb-ssd-hs3-linu/FM_PROJECT/FMDL_3.1/cable/boxDetector_V2_multiclass_Cable/try1/dataFor__boxDetector_V2_multiclass_cable__try1/firstTry_final.csv'\n",
    "\n",
    "\n",
    "existingRows = readCsvRows(csvToWorkWith)\n",
    "\n",
    "n1 = 0\n",
    "for row in existingRows:\n",
    "    vals = row.split(',')\n",
    "\n",
    "    if vals[0] not in imIdsToDelete:\n",
    "        existingRows.remove(row)\n",
    "        n1 += 1\n",
    "        \n",
    "print(\"in the first run deleted \" + str(n1) +' rows\\n')\n",
    "\n",
    "n2 = 0\n",
    "for row in existingRows:\n",
    "    vals = row.split(',')\n",
    "\n",
    "    if vals[0] not in imIdsToDelete:\n",
    "        existingRows.remove(row)\n",
    "        n2 += 1\n",
    "        \n",
    "print(\"in the second run deleted \" + str(n2) +' rows\\n')\n",
    "print(\"deleted \" + str(n1+n2) + \" rows in total\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# open the file\n",
    "csv_file = open(csvToWorkWith, \"w\") \n",
    "\n",
    "# define column names\n",
    "columnTitles = \"filename,pathname,xmins,xmax,ymins,ymax,class\\n\"\n",
    "csv_file.write(columnTitles)\n",
    "\n",
    "# write rows\n",
    "for r in existingRows:\n",
    "    row = r + '\\n'\n",
    "    csv_file.write(row)\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"wrote \" + str(len(existingRows)) + \" rows to csv file\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# add no bucket rows to existing csv and shuffle its rows\n",
    "\n",
    "csvRows = readCsvRows('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try5/trainSet_multiClass_bucket_fineRock_try5_manuallyCleaned.csv')\n",
    "\n",
    "#getRid of the empty row at the end\n",
    "csvRows = csvRows[0:len(csvRows)-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add the no bucket rows for the images in dir\n",
    "for imId in os.listdir('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try6/noShovelImagesToAdd/'):\n",
    "    newRow = str(str(imId) + ',' + str(imagesPath) + str(imId) + ',' + '' + ',' + '' + ',' + '' + ',' + '' + ',' + '')\n",
    "    print(newRow)\n",
    "    \n",
    "    csvRows.append(newRow)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# shuffle the rows\n",
    "from random import shuffle\n",
    "shuffle(csvRows)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#write the shuffled rows to csv\n",
    "csv_file = open('/home/hooman/ssdMobileNet_multiClass_bucket_rockInside_FineInside_NoTeeth_NoCase/try5/trainSet_multiClass_bucket_fineRock_try5_manuallyCleaned_new.csv', \"w\") \n",
    "\n",
    "\n",
    "# write rows\n",
    "for row in csvRows:\n",
    "    csv_file.write(row + '\\n')\n",
    "\n",
    "csv_file.close()\n",
    "\n",
    "print(\"wrote \" + str(len(csvRows)) + \" rows to csv file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# remove matInsideBoundary rows from CSV\n",
    "\n",
    "rowsDic = getCertainClassRowsDictFromCsv('/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/trainSet_bucketAndMatInsideBoundaries_allImages_BucAndPnH_cleaned.csv', ['bucket'])\n",
    "\n",
    "writeRowDicToCsv(rowsDic, '/home/hooman/dataPreparation/hsTrainingSetBucyrusAndPnH/unet/trainSet_justBucketBoundaries_allImages_BucAndPnH_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
